\section{Universal Differential Equations}

Because of the scarcity of data in many scientific domains, mechanistic models are frequently employed instead of deep learning.
These models often come in the form of \glspl{ODE} or \glspl{PDE} that describe the system's behavior over time, referred to as the system's dynamics.
Mechanistic models are explainable, based on prior structural knowledge, and backed by many years of rigorous studies, but they lack the flexibility of deep learning models and usually make unrealistic assumptions about real-world phenomenons.
In contrast, deep learning models are highly flexible, but they require a massive amount of data to be effective.
Based on those observations, many researchers have recently presented with different methods for merging machine learning and mechanistic models.

As introduced in \autoref{sec:literature-review-physics-informed-neural-network}, \glspl{PINN} can enforce physical constraints on \glspl{ANN} through the use of \glspl{PDE}/\glspl{ODE} as a regularization term in the loss function.
\glspl{PINN} were shown to have high performance for some scientific applications with limited data.
However, it still lacks the interpretability of  mechanistic models even though it can be seen that the \gls{ANN} has learned real world physical characteristics.
Instead of incorporating scientific knowledge into the model, \glspl{NeuralODE} take a different approach by using the structure of scientific models as a basis for machine learning.
While \glspl{NeuralODE} can learn the continuous dynamics data, as shown in \autoref{sec:literature-review-neural-ordinary-differential-equations}, the resulting models do not represent any known mechanisms.
To address these issues, \gls{UDE} was proposed which extended the approach taken by \glspl{NeuralODE} \cite{rackauckasUniversalDifferentialEquations2020}.
\glspl{UDE} apply mechanistic modeling in conjunction with universal approximators where part of the differential equation embeds a universal approximator, e.g., a neural network, a random forest, or a Chebyshev expansion.
This combination helps create a robust model, where well-proven terms in the mechanistic model can stay unchanged, while the terms with complex unknown interactions can be discovered by the universal approximator.

In general, a \gls{UDE} model will be in the form of \cite{rackauckasUniversalDifferentialEquations2020}
\begin{equation}
    u' = f(u, t, U_\theta(u, t)),
\end{equation}
where $f$ denotes a known mechanistic model whose missing terms are defined by some universal approximator $U_\theta$.
The process for training \glspl{UDE} is similar to training typical \glspl{ANN} a loss function $\mathcal{L}(\theta)$, defined for the approximated solution $u_\theta(t)$ with respect to the current choice of parameters $\theta$, is minimized.
Common choices for a loss function used to train \glspl{ANN} are applicable in this case, such as the \gls{MSE} $\mathcal{L}(\theta) = \sum_{i=1}^{N} (u_\theta(t_i) - d_i)^2$ at discrete data points $\{(t_i, d_i)\}_{i=i}^N$.
Then the gradients of the loss with respect to the parameters $\frac{\partial\mathcal{L}}{\partial\theta}$ are calculated for applying local gradient-based methods such as gradient descent.
Methods for computing the gradients of \glspl{UDE} of different types of differential equations are implemented as a library in the Julia programming language \cite{bezanson2012julia}, which will be utilized in this thesis \cite{rackauckasUniversalDifferentialEquations2020}.