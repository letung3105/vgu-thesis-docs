\section{Software and hardware}

We implemented the scripts for generating the datasets described in \autoref{sec:methodologies-data}, the models defined in \autoref{sec:methodologies-models-definitions} and the training algorithm defined in \autoref{sec:methodologies-parameters-optimization} using the Julia programming language \cite{bezanson2012julia}.
The implementation was written as a Julia package and was tested with Julia version 1.6.3.
We chose Julia for the implementation because of the strongly supported packages for solving differential equations and computing the gradients of those equations.
The two main packages that we utilized are the \textit{DifferentialEquations} package \cite{rackauckas2017differentialequations} for solving the system of \glspl{ODE} defined by \autoref{eq:methodologies-seir-ude-model} and the \text{DiffEqFlux} \cite{rackauckasUniversalDifferentialEquations2020} package for computing the gradients and optimizing the system's parameters.

We conducted the experiment on two different hardware configurations.
The first that we used for testing the models was the cloud computing instance provided by Google Collab \footnote{\url{https://colab.research.google.com}}.
The provided system ran on the Ubuntu 18.04 operating system and included a 2 cores Intel(R) Xeon(R) CPU with each core running at 2.30GHz, and 12Gb of memory.
The second hardware system that we used was our personal laptop.
The laptop ran on the Manjaro operating system with Linux kernel version 5.10.70-1-MANJARO and included a 2 cores Intel(R) Core(TM) i5-4260U CPU with each core running at 1.40GHz, and 4Gb of memory.
While our models were expected to run on any operating system that was supported by Julia, we did not test them on any other operating systems besides Linux.
Thus the model might not work as expected when running on a different operating system.
Although we had tested the model and shown that it could run on a resource-limited system with only 4Gb memory, we recommended using a system with at least 8Gb for the best user's experience.
Because the Julia programming language utilizes \gls{JIT} compilation every time we run a line of code, the initial startup of our package can consume lots of hardware resources and crashes the program if there is not enough memory for the \gls{JIT} compilation process.
After the initial startup process is completed, the training process and evaluation process do not require as much hardware resources.